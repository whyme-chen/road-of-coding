# 数据采集与清洗

包括实时数据采集、数据抓取、数据预处理等技术，用于从各种数据源中收集和清洗数据。

* [博客社区](https://www.kdnuggets.com/)
* [在线JSON校验格式化工具](https://www.bejson.com/)

## 网络爬虫

### 概述

1. 爬虫：全称应为网络爬虫，搜索引擎就是网络爬虫的应用。爬虫本质上就是一种**按照一定规则，自动你抓取互联网信息的程序或脚本**。

2. 应用

   * 零售业
   * 金融行业
   * 数据分析/人工数据集

3. 核心及构建流程

   爬虫的核心主要分为两部分，第一部分是爬取网页，第二部分就是解析数据。具体流程可分为以下步骤：

   * 确定目标网站，这个目标网站就是你要获取数据的网站。
   * 确定你要获取数据的页面的 URL。
   * 对第 2 步中的 URL 发起请求以获取页面的 HTML 代码。
   * 使用解析工具从 HTML 中获取想要的数据。
   * 将第 4 步中获取到的数据保存到 JSON 或 CSV 文件中或者保存成其他格式。

4. 背景调研（合法使用爬虫）

   在深入讨 论爬取一个网站 之前，我们首先需要对目标站点的规模和结构进行一定程度的了解。例如网站自身的robots.txt和Sitemap文件

   * 检查robots.txt

   * Sitemap

   构建网站 所使用的技术类型也会对我们如何爬取产生影响。 有一个十分有用的工具可以检查网站 构建的技术类型一－builtwith 模块。

   下载该模块

   ~~~python
   pip install builtwith
   ~~~

   使用该模块对URL进行分析

   ~~~python
   import builtwith
   builtwith.parse('要进行分析的URL')
   ~~~

5. 本质：模拟浏览器打开网页，获取网页中我们想要的部分数据。

6. 分类

   * 通用爬虫：常见的就是**搜索引擎**，无差别的收集数据、存储，提取关键字，构建索引库，给用户提供搜索接口。

     

   * 聚焦爬虫：根据需求，实现爬虫程序，抓取需要的数据。

7. 反爬手段

   * User- Agent：用户代理，简称UA,它是一个特殊字符串头， 使得服务器能够识别客户使用的操作系统及版本、CPU类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等。

   * 代理IP：

     西次代理
     快代理
     什么是高匿名、匿名和透明代理?它们有什么区别?
     使用透明代理，对方服务器可以知道你使用了代理，并且也知道你的真实IP。
     使用匿名代理，对方服务器可以知道你使用了代理，但不知道你的真实IP。
     使用高匿名代理，对方服务器不知道你使用了代理，更不知道你的真实IP。

   * 验证码访问

   * 动态加载网页网站返回的是js数据 并不是网页的真实数据
     selenium驱动真实的浏览器发送请求

   * 数据加密
     分析js代码

### HTTP协议

> 参考road-of-coding

1. url组成
   * 协议：http，https
   * 主机
   * 端口号：http协议对应端口号为80，https协议对应端口号为443
   * 路径
   * 参数
   * 锚点

#### 请求响应过程

### web前端

> 参考web知识.md

### urllib库

urllib库是python内置的用于进行URL请求的标准库。它可以帮助我们实现HTTP、FTP、SMTP等协议的访问和操作，并且非常简单易用。

urllib库包含4个模块：

* `urllib.request`
* `urllib.error`
* `urllib.parse`
* `urllib.robotparser`

`urllib.request`模块用于打开和读取URL，支持发送各种类型的请求，如GET、POST、PUT等，以及添加Header和参数等。`urllib.parase`用于对数据进行编码解码转换。以下是常用的方法示例：

~~~python
import urllib.request
import urllib.parse

# url = "http://www.baidu.com"

# response = urllib.request.urlopen(url)
#
# # 可以在read方法中设置读取的字节长度
# content = response.read().decode("utf-8")
#
# print(content)

# content = response.readLine().decode("utf-8")

# content = response.readLines().decode("utf-8")

# print(response.geturl())
# print(response.getcode())
# print(response.getheads)

# 下载文件
# urllib.request.urlretrieve("https://img2.baidu.com/it/u=352808565,3906628293&fm=253&fmt=auto&app=138&f=JPEG?w=500&h=555","御坂美琴.jpg")

url = "https://www.baidu.com"

headers = {
"User-Agent":"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Mobile Safari/537.36"
}

# get请求的编码
# name = urllib.parse.quote("御坂美琴")
# print(name)

# data = {
#     "name":"御坂美琴",
#     "age":20,
#     "gender":"女"
# }
# name = urllib.parse.urlencode(data)
# print(name)

# 构建请求对象
request = urllib.request.Request(url, headers=headers)

response = urllib.request.urlopen(request)

content = response.read().decode("utf-8")

print(content)
~~~

获取豆瓣排行榜数据：

```python
"""
爬取豆瓣电影排行榜前十页的数据，并将数据写入到excel文件中
"""
import urllib.request
import urllib.parse
from time import sleep

import openpyxl
import json

# url地址
url = "https://movie.douban.com/j/chart/top_list"
headers = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive",
    "Host": "movie.douban.com",
    "Pragma": "no-cache",
    "Referer": "https://movie.douban.com/chart",
    "Sec-Ch-Ua-Mobile": "?1",
    "Sec-Ch-Ua-Platform": "Android",
    "Cookie": "bid=vWfVWxiUZnQ; douban-fav-remind=1; ll='118337'; __yadk_uid=TtfOG2xzF6w62ZzjLmp0FLWYDpk24Z2c; _vwo_uuid_v2=D18926FD84857F11CC2CB504AE3071036|2e4fe5d34b93354349017bb1be82fa55; _pk_ref.100001.4cf6=%5B%22%22%2C%22%22%2C1694253306%2C%22https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3D8N74t3DU7_lOZaQvDx_Szt_ufsEL2lcsK_VBKdamUHSLzDLv6lSH4n84TAFvGCYA%26wd%3D%26eqid%3D9ce3be50005ba9a50000000364fc40f5%22%5D; _pk_id.100001.4cf6=166e3cd4ded8a4f8.1682854787.; _pk_ses.100001.4cf6=1; ap_v=0,6.0; __utma=30149280.1105229747.1694253306.1694253306.1694253306.1; __utmb=30149280.0.10.1694253306; __utmc=30149280; __utmz=30149280.1694253306.1.1.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; __utma=223695111.424085783.1682854788.1682854788.1694253306.2; __utmb=223695111.0.10.1694253306; __utmc=223695111; __utmz=223695111.1694253306.2.2.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; Hm_lvt_16a14f3002af32bf3a75dfe352478639=1694253336; Hm_lpvt_16a14f3002af32bf3a75dfe352478639=1694253336",
    "User-Agent": "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Mobile Safari/537.36"
}

# 获取电影列表
movie_list = []
for i in range(3):
    params = {
        "type": 11,
        "interval_id": "100:90",
        "action": "",
        "start": i * 20,
        "limit": 20
    }

    # 将params中的参数和值拼接到URL上
    query_string = "&".join([f"{key}={urllib.parse.quote(str(value))}" for key, value in params.items()])
    new_url = f"{url}?{query_string}"
    print(new_url)
    # 获取网页内容
    request = urllib.request.Request(new_url, headers=headers)
    response = urllib.request.urlopen(request)
    html_content = response.read().decode('utf-8')
    # print(html_content)
    movie_list += json.loads(html_content)
    # 每次请求后暂停1s
    sleep(1)

print(len(movie_list))
print(movie_list)

# # 创建Excel文件
# wb = openpyxl.Workbook()
# ws = wb.active
# # 设置表头
# ws['A1'] = "排名"
# ws['B1'] = "电影名字"
# ws['C1'] = "类型"
# ws['D1'] = "国家"
# ws['E1'] = "封面"
# ws['F1'] = "上映时间"
# ws['G1'] = "地址"
# ws['H1'] = "评分"
# ws['I1'] = "演员"
#
# index = 0
# # 逐个处理每一部电影
# for movie in movie_list:
#     ws.cell(row=index + 2, column=1, value=movie["rank"])
#     ws.cell(row=index + 2, column=2, value=movie["title"])
#     ws.cell(row=index + 2, column=3, value=",".join(movie["types"]))
#     ws.cell(row=index + 2, column=4, value=",".join(movie["regions"]))
#     ws.cell(row=index + 2, column=5, value=movie["cover_url"])
#     ws.cell(row=index + 2, column=6, value=movie["release_date"])
#     ws.cell(row=index + 2, column=7, value=movie["url"])
#     ws.cell(row=index + 2, column=8, value=movie["score"])
#     ws.cell(row=index + 2, column=9, value=",".join(movie["actors"]))
#     index = index + 1
#     print(f"{movie['title']}")
#
# # 保存Excel文件
# wb.save("豆瓣电影剧情类排行榜.xlsx")
# print("数据获取完毕")
```

`urllib.error`提供了处理与 URL 相关的错误的功能。该模块包含一些异常类，用于捕获和处理在使用 `urllib.request` 发送请求时可能出现的错误。

`urllib.error` 模块中常用的异常类：

* `URLError`：这是 `urllib.error` 模块中最常见的异常类，用于捕获与 URL 请求相关的错误。它是 `OSError` 的子类，并具有一些特定于 URL 请求的属性和方法。例如，可以使用 `reason` 属性获取发生错误的原因。
* `HTTPError`：这是 `URLError` 的子类，专门用于捕获 HTTP 请求错误。当发生 HTTP 错误（如 404 Not Found 或 500 Internal Server Error）时，会抛出此异常。通过该异常类，可以获取响应状态码、错误原因以及相应的响应头部信息。
* `ContentTooShortError`：该异常类用于捕获请求的内容过短的错误。当使用 `urllib.request.urlretrieve()` 下载文件时，如果下载的文件大小少于指定的大小，就会抛出此异常。

在处理 `urllib` 请求时，通常需要使用 `try-except` 语句块来捕获可能发生的异常。以下是一个简单的示例：

```python
import urllib.request
from urllib.error import URLError, HTTPError

url = "http://www.example.com/"

try:
    response = urllib.request.urlopen(url)
    # 处理响应
except HTTPError as e:
    print("HTTP Error:", e.code, e.reason)
except URLError as e:
    print("URL Error:", e.reason)
except Exception as e:
    print("Error:", str(e))
```

### Jsoup

#### Element类

1. 简介：`org.jsoup.nodes.Element`类是Jsoup中比较重要的一个类，它表示HTML文档中的一个元素。

2. 常用方法：

   > 获取元素信息
   >
   > - `String tagName()`: 获取元素的标签名，例如`div`、`a`等。
   > - `String text()`: 获取元素的文本内容，不包括子元素的内容。
   > - `String html()`: 获取元素的HTML内容，包括子元素的内容。
   > - `Attributes attributes()`: 获取元素的属性集合。
   >
   > 查找子元素
   >
   > - `Element child(int index)`: 返回指定索引位置的子元素。
   > - `Elements children()`: 返回所有子元素的集合。
   > - `Elements children(String tagName)`: 返回指定标签名的子元素集合。
   > - `Element selectFirst(String cssQuery)`: 返回匹配CSS选择器的第一个子元素。
   > - `Elements select(String cssQuery)`: 返回匹配CSS选择器的所有子元素的集合。
   >
   > 查找父元素和兄弟元素
   >
   > - `Element parent()`: 返回当前元素的父元素。
   > - `Elements parents()`: 返回当前元素的所有祖先元素的集合。
   > - `Element previousElementSibling()`: 返回前一个同级元素。
   > - `Element nextElementSibling()`: 返回后一个同级元素。
   >
   > 操作元素
   >
   > - `void text(String text)`: 设置元素的文本内容。
   > - `void html(String html)`: 设置元素的HTML内容。
   > - `Element attr(String attributeKey, String attributeValue)`: 设置元素的属性。
   > - `Element removeAttr(String attributeKey)`: 删除元素的指定属性。
   > - `Element append(String html)`: 在元素内部的最后位置添加指定内容。
   > - `Element prepend(String html)`: 在元素内部的最前位置添加指定内容。
   > - `Element before(String html)`: 在元素前面插入指定内容。
   > - `Element after(String html)`: 在元素后面插入指定内容。
   >
   > 以上是Element类中一些常用的方法，可以满足我们对HTML文档的大部分操作需求。

### Scrapy框架

# 数据存储与管理

包括分布式文件系统（如Hadoop HDFS）、NoSQL数据库（如MongoDB、Cassandra）、列式数据库（如HBase）等，用于高效地存储和管理大量数据。

# 数据处理与计算

包括分布式计算框架（如Apache Spark、Apache Flink）、数据流处理引擎（如Apache Kafka、Apache Storm）等，用于对大规模数据进行处理和计算。

# 数据分析与挖掘

包括统计分析、机器学习、深度学习等技术，用于从海量数据中提取有价值的信息和知识。

## 数据预处理

包括数据清洗、数据集成、数据转换和数据归约，旨在准备和清理原始数据以便于下一步分析

## 数据建模及模型评估

使用各种机器学习算法（如分类、聚类、回归、关联规则等）对数据进行建模和分析，以发现模式、关联和趋势。

# 数据可视化与报表

使用图表、图形和可交互的界面将数据可视化，帮助用户更好地理解数据的结构、趋势和关系，以及挖掘的结果。

# 数据安全与隐私保护

包括数据加密、访问控制、身份验证等技术，用于保护大数据的安全性和隐私性。